{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy.io as io\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter \n",
    "from scipy.ndimage import gaussian_filter\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from matplotlib import cm as CM\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(label_info, image_shape, dataset):\n",
    "    \"\"\"\n",
    "    Generate a density map based on objects positions.\n",
    "    Args:\n",
    "        label_info: list of (x, y, ...) objects positions\n",
    "        image_shape: (width, height) of a density map to be generated\n",
    "        dataset: one of {SHT, JHU}\n",
    "    Returns:\n",
    "        A density map.\n",
    "    \"\"\"    \n",
    "    # create an empty density map\n",
    "    density = np.zeros(image_shape, dtype=np.float32)\n",
    "\n",
    "    if dataset == 'SHT':\n",
    "        for x, y, *_ in label_info:\n",
    "            if int(y) < image_shape[0] and int(x) < image_shape[1]:\n",
    "                density[int(y)][int(x)] = 1\n",
    "    elif dataset == 'JHU':\n",
    "        for string in label_info:\n",
    "            line = string.split()\n",
    "            x, y = line[0], line[1]\n",
    "            if int(y) < image_shape[0] and int(x) < image_shape[1]:\n",
    "                density[int(y)][int(x)] = 1\n",
    "        \n",
    "    # apply a convolution with a Gaussian kernel\n",
    "    density = gaussian_filter(density, sigma=(1, 1), order=0)\n",
    "\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating consolidated HDF5 files for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHT Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HDF5 files...\n",
      "Done: Generating HDF5 files for 10 instance(s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set the root to the Shanghai dataset you download\n",
    "root = '../data/SHT/'\n",
    "\n",
    "part_A_train = os.path.join(root,'part_A_final/train_data','images')\n",
    "part_A_test = os.path.join(root,'part_A_final/test_data','images')\n",
    "path_sets = [part_A_train,part_A_test]\n",
    "\n",
    "img_paths = []\n",
    "for path in path_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "print('Generating HDF5 files...'.format(len(img_paths)))\n",
    "for id, img_path in enumerate(img_paths):\n",
    "    #print('Currently at file {}'.format(id+1))\n",
    "    gt = io.loadmat(img_path.replace('.jpg','.mat')\n",
    "                     .replace('images','ground_truth')\n",
    "                     .replace('IMG_','GT_IMG_'))\n",
    "    gt = gt['image_info'][0][0][0][0][0]\n",
    "    \n",
    "    img = plt.imread(img_path) / 255\n",
    "    label = generate_label(gt, img.shape[:2], 'SHT')\n",
    "    \n",
    "    with h5py.File(img_path.replace('.jpg','.h5').replace('images','consolidated'), 'w') as hdf:\n",
    "        hdf['image'] = img\n",
    "        hdf['label'] = label\n",
    "        \n",
    "print('Done: Generating HDF5 files for {} instance(s)'.format(id+1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHT Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HDF5 files...\n",
      "Done: Generating HDF5 files for 10 instance(s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set the root to the Shanghai dataset you download\n",
    "root = '../data/SHT/'\n",
    "\n",
    "part_B_train = os.path.join(root,'part_B_final/train_data','images')\n",
    "part_B_test = os.path.join(root,'part_B_final/test_data','images')\n",
    "path_sets = [part_B_train,part_B_test]\n",
    "\n",
    "img_paths = []\n",
    "for path in path_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "print('Generating HDF5 files...'.format(len(img_paths)))\n",
    "for id, img_path in enumerate(img_paths):\n",
    "    gt = io.loadmat(img_path.replace('.jpg','.mat')\n",
    "                     .replace('images','ground_truth')\n",
    "                     .replace('IMG_','GT_IMG_'))\n",
    "    gt = gt['image_info'][0][0][0][0][0]\n",
    "    \n",
    "    img = plt.imread(img_path) / 255\n",
    "    label = generate_label(gt, img.shape[:2], 'SHT')\n",
    "    \n",
    "    with h5py.File(img_path.replace('.jpg','.h5').replace('images','consolidated'), 'w') as hdf:\n",
    "        hdf['image'] = img\n",
    "        hdf['label'] = label\n",
    "        \n",
    "print('Done: Generating HDF5 files for {} instance(s)'.format(id+1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HDF5 files...\n",
      "Done: Generating HDF5 files for 15 instance(s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set the root to the Shanghai dataset you download\n",
    "root = '../data/JHU/'\n",
    "\n",
    "JHU_train = os.path.join(root, 'train', 'images')\n",
    "JHU_test = os.path.join(root, 'test', 'images')\n",
    "JHU_val = os.path.join(root, 'val', 'images')\n",
    "\n",
    "path_sets = [JHU_train, JHU_test, JHU_val]\n",
    "\n",
    "img_paths = []\n",
    "for path in path_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "print('Generating HDF5 files...'.format(len(img_paths)))\n",
    "for id, img_path in enumerate(img_paths):\n",
    "    gt = open(img_path.replace('.jpg', '.txt')\n",
    "              .replace('images', 'gt'), 'r').readlines()\n",
    "    \n",
    "    img = plt.imread(img_path) / 255\n",
    "    label = generate_label(gt, img.shape[:2], 'JHU')\n",
    "    \n",
    "    with h5py.File(img_path.replace('.jpg','.h5').replace('images','consolidated'), 'w') as hdf:\n",
    "        hdf['image'] = img\n",
    "        hdf['label'] = label\n",
    "        \n",
    "print('Done: Generating HDF5 files for {} instance(s)'.format(id+1))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
